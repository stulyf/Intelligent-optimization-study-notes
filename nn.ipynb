{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前一些关于神经网络的没能理解的知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题一**\n",
    "\n",
    "激活函数中的 “ 激活 ” 该如何理解 ？\n",
    "\n",
    "激活函数的“激活”源于对生物神经元的模拟：\n",
    "\n",
    "生物神经元：接收到足够的电信号（超过阈值）后才会“激活”，传递信号。\n",
    "人工神经元：当输入信号的加权和（z=w⋅x+b）通过激活函数后，若输出值较大（如ReLU在z>0时输出z），视为该神经元被“激活”，否则被“抑制”（如ReLU在z≤0时输出0）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题二**\n",
    "\n",
    "激活函数如何使网络拟合复杂函数 ？ \n",
    "\n",
    "无激活函数：多层网络等价于单层线性变换。\n",
    "例如：y = W2 (W1 * x + b1) + b2 = W2*W1 * x + (W2 *b1 + b2 )，仍是线性函数。\n",
    "加入激活函数：每层通过非线性函数映射，多层叠加后可逼近任意复杂函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*问题三*\n",
    "\n",
    "如何通过链式法则计算损失对权重w的梯度 ？\n",
    "\n",
    "前向传播 ： 就是通过逐层神经网络 从输入层 到 中间层 到 输出层 将输入映射为输出的过程\n",
    "\n",
    "反向传播 ：就是 计算损失函数对每一层权重的偏导 这个过程通过链式法则计算 ：\n",
    "               将损失结果对某一层的权重的偏导 转换为 损失结果对前一层结果的偏导 前一层结果对再前一层的偏导 ...... 该层结果对该层权重的偏导\n",
    "               \n",
    "通过计算出损失函数对每一层权重的偏导，我们可以沿着偏导下降的方向更新参数，更新参数的幅度可以表示为学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*问题四*\n",
    "\n",
    "怎么直观理解卷积层中的“卷积”？\n",
    "\n",
    "卷积相当于图像处理中的滤波器 ： 卷积操作类似于用一个小窗口，在图像上滑动，逐块提取局部特征\n",
    "\n",
    "每个神经元只连接输入图像的局部区域，同一个滤波器在整个图像上滑动使用，减少参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*问题五*\n",
    "\n",
    "什么是"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
