{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "团队定向越野问题\n",
    "\n",
    "1. Multi-Agent Reinforcement Learning for the Vehicle Routing Problem\n",
    "\n",
    "2.\n",
    "\n",
    "随机定向越野问题 ：\n",
    "\n",
    "1. Risk-Averse Stochastic Orienteering\n",
    "\n",
    "动态定向越野问题 ：\n",
    "\n",
    "1. Reinforcement Learning for Dynamic Orienteering Problems\n",
    "\n",
    "多目标定向越野问题：\n",
    "\n",
    "1.Pareto Monte Carlo Tree Search for Multi-Objective Orienteering\n",
    "\n",
    "综述 ：\n",
    "\n",
    "1. The Orienteering Problem: A Survey\n",
    "\n",
    "2.动态与随机OP的最新进展 ：Dynamic and Stochastic Orienteering Problems: Models and Algorithms\n",
    "\n",
    "3.基于深度学习的OP研究综述 ：Deep Learning for Combinatorial Optimization in Routing Problems: A Survey\n",
    "\n",
    "4. 多目标与绿色OP的前沿方向 ： Multi-Objective Orienteering Problems: A Survey of Models and Applications\n",
    "\n",
    "创新点 ：结合图神经网络（GNN）与强化学习，建模动态节点关系/开发基于事件触发的实时重规划策略/基于注意力权重的路径决策可视化/结合符号推理与神经网络的混合模型/多目标优化框架：平衡收益、时间与碳排放\n",
    "\n",
    "关注点 ： 设计合适的奖励函数，状态表示，智能体之间的协作/竞争机制 ， 模仿学习 ， drl和传统算法结合\n",
    "\n",
    "\n",
    "想法 ：\n",
    "1.模仿学习/元学习 ： 找定向越野运动员的运动路线GPS数据集 -》 作为专家模型 -》 通过这个专家模型来指导深度强化学习模型\n",
    "\n",
    "2. 为定向越野问题设置量身定制的新型奖励函数和状态表示\n",
    "\n",
    "3.探索使用多智能体强化学习解决多参与者定向越野场景、\n",
    "\n",
    "4.将先进的深度学习架构（如Transformer和GNN）应用于定向越野问题\n",
    "\n",
    "5.能够实时适应动态和随机定向越野问题环境的DRL模型/为实际定向越野问题应用开发基于DRL的决策支持系统\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
