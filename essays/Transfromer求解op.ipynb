{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title :SOLVING THE TEAM ORIENTEERING PROBLEM WITH TRANSFORMERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "补充内容 OP问题 \n",
    "\n",
    "OP问题属于vrp问题的一个分支，对于每个节点，我们对其定义不同的奖励和收益，我们需要规划一条路径，使其在有限制的资源约束下，获得尽可能大的收益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "摘要部分 ： \n",
    "\n",
    "解决的问题 ： TOP 问题，团队OP问题，要求agent之间要相互协作\n",
    "\n",
    "问题背景 ：  主流求解方法是线性规划，但是计算时间随着时间成本的增加而增加 ； 或者是启发式方法，在较短时间内能够找到次优解\n",
    "\n",
    "解决思路 ： 采用集中式Transformer的全新方法，将整个场景编码成图结构\n",
    "\n",
    "\n",
    "\n",
    "Q ： 启发式方法和强化学习的关系 ？ 什么是 “集中式”Transformer？如何编码成图结构？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引言部分 ： \n",
    "\n",
    "背景部分，提到多代理路线规划算法需要考虑问题的全局视图，从而平衡每个代理的工作负荷，从而最大化效率\n",
    "\n",
    "问题部分，提到与OP问题相比，TOP问题因为存在多个代理而更加复杂\n",
    "\n",
    "现有方法，有方法提到通过将场景区分为互斥区域，来将top问题转换为多个独立op问题，也有方法提出了区域共享策略允许多个代理在特定条件下访问同一区域，以往算法的全局性不太理想，因为每个代理不能考虑其他代理的决策，代理之间的合作也有限，只有部分机制尝试回复损失的性能\n",
    "\n",
    "论文提出的方法，Transformer 网络设计，节点选择机制，为所有代理预测路径而不是顺序处理，通过掩码技术确保满足约束条件的节点，如同时间限制，和单次访问限制；训练的时候使用采样策略分布；推理的时候，使用贪婪选择，选择概率最高的节点；强化学习框架下使用reinforce优化网格的参数，reinforce就是通过计算梯度，更新参数来实现\n",
    "\n",
    "Q ： 掩码技术是如何实现对约束条件的限制的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A  ：\n",
    "启发式方法是通过人工设计规则或者是策略，对问题领域的求解，强化学习是通过和环境不断交互实现自动学习启发式规则的方法\n",
    "\n",
    "分散式，每个agent有自己的决策模型 ；集中式，单一控制系统同时为所有的代理做决策\n",
    "\n",
    "实现图结构编码，首先给每个节点通过坐标和奖励值进行编码，然后对起点和终点用特殊嵌入表示，通过线性投影将原始特征转换成高维，每个节点和其他节点的关联性通过transformer计算节点之间的关系矩阵\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
