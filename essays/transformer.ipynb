{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer模型详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种完全基于注意力机制构建的，支持并行处理的深度学习模型架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是注意力机制？\n",
    "\n",
    "注意力机制指的是在处理序列数据的时候，模型不需要同等对待输入的所有元素，而是只关注最重要的元素，这个通过计算不同输入元素的权重来实现，权重越高代表元素重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 基本架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 有编码器和解码器组成，编码器用于处理输入，解码器用于处理输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器(Encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 由N个相同的层堆叠而成，每一层的输出作为下一层的输入，这样的设计能够运行更加抽象、更高级的特征表示\n",
    "\n",
    "* 每层的编码器包含两个子层：\n",
    "\n",
    "- 多头自注意力机制： 输入序列中的每个元素都被转化为查询(Q)、键(K)和值(V)三种向量,通过计算查询和键的点积，确定词与词之间的关注度，这些关注度决定了每个位置该从其他位置汲取信息\n",
    "\n",
    "- 前馈神经网络 ：  FFN(x) = W₂(ReLU(W₁x + b₁)) + b₂\n",
    "\n",
    "其中relu激活函数的作用是引入非线性，这里的relu激活函数为ReLU(x) = max(0, x)，简单且有效\n",
    "这里的第一层将维度从d_model扩展为d_ff,第二层将维度从d_ff压缩会d_model,这个从低维度映射到高维度的操作，可以将低维度中很多线性不可分的问题转化为线性可分\n",
    "\n",
    "* 每个子层都使用残差连接和层归一化\n",
    "\n",
    "  * 残差连接的数学表达：输出 = 子层(输入) + 输入\n",
    "\n",
    "  这个残差连接能够解决梯度消失问题\n",
    "\n",
    "  * 层归一化 ； 每个特征都被归一化，能够稳定激活值分布，防止极大或极小\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器（Decoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器由N个相同的层堆叠而成，每个层包含三个子层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 掩码多头自注意力机制 ： 标准多头自注意力机制的变体，就是添加了未来信息屏蔽的功能，保证在生成序列时，当前位置只能看见自己及之前的信息\n",
    "\n",
    "    softmax函数应用在这个机制之后，未来位置的注意力权重处理为0\n",
    "\n",
    "    softmax函数能够将每个实数变量转换成概率分布的数学函数，即先对每个函数取指数，然后除以所有元素指数的总和进行归一化\n",
    "\n",
    "* 编码器-解码器注意力机制 ；查询(Q)来自解码器前一层输出，键(K)和值(V)来自编码器最终输出。\n",
    "\n",
    "* 前馈神经网络\n",
    "\n",
    "* 同样使用残差连接和层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自注意力机制的完整公式：\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
