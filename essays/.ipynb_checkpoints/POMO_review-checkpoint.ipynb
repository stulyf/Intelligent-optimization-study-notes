{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title : POMO: Policy Optimization with Multiple Optima for Reinforcement Learning\n",
    "\n",
    "多重最优策略优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "摘要部分： \n",
    "\n",
    "背景知识，强化学习可以将深度神经网络转换成快速的启发式求解器，\n",
    "\n",
    "算法属性，pomo是一种端对端的方法（就是无需人工干预），和am一样，适合广泛的组合优化问题，其利用修改的reinforce算法，对所有解进行不同程度的展开，采取了低方差基线，使得学习训练更加快速稳定，引入了新的基于增强的推理方法，和pomo完美配合？\n",
    "\n",
    "算法结果，通过tsp，cvrp，kp验证了pomo的有效性，最终的结果是求解器优于所有近期的学习型启发式算法，在tsp100上，实现了0.14%最优性差距\n",
    "\n",
    "Q  ： 什么是低方差基线？ 摘要中的对称性指的是什么？基于增强的推理方法具体指什么？如何实现增强？\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结尾部分 ；\n",
    "\n",
    "pomo本质 ： 纯数据驱动的组合优化方法 不需要手动的启发式规则\n",
    "\n",
    "pomo工作原理核心  ：利用组合优化中多重最优解的存在，在训练和推理过程中，都能够高效引领模型向最优解靠拢\n",
    "\n",
    "pomo结果  ：pomo方案接近理论最优解，减少推流时间，在构建型方案中有优势\n",
    "\n",
    "q ： 多重最优解  ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读完开头结尾的q ： \n",
    "\n",
    "既然本文章提出的pomo是基于数据驱动的方案，那么我想知道它的数据来源是生成式数据，还是真实数据，如果是生成式数据的话，他是按照什么方法找到理论最优解的，还有就是如果是生成式数据的话，本项目有没有实际意义？如果是真实数据，那么数据量有多少，能够支撑起深度学习吗\n",
    "\n",
    "端对端特征构建的可解释性 ？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detailed reading  ；\n",
    "\n",
    "几个核心概念 or trick ： \n",
    "\n",
    "多重最优解 ？ 像是v1→v2→v3→v4→v5和v2→v3→v4→v5→v1，内部节点的顺序不变，首尾节点发生变化\n",
    "\n",
    "多起点并行探索  ：不在使用单一起点，从多个起点并行生成轨迹，同时参与更新模型\n",
    "\n",
    "上面探索模式的更新方式就是 ：∇θJ(θ) ≈ (1/N) * Σ[(R(τi) - bshared(s)) * ∇θlog pθ(τi|s)]\n",
    "\n",
    "相当于对所有轨迹的汇报进行了平均化处理，这个方式就解决了数据量问题\n",
    "\n",
    "算法过程就是 ：\n",
    "\n",
    "采样B个问题实例\n",
    "\n",
    "为每个实例选择N个起点\n",
    "\n",
    "生成B×N个解决方案轨迹\n",
    "\n",
    "每个实例计算N个轨迹的平均回报作为该实例的基线\n",
    "\n",
    "使用策略梯度更新网络参数\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rl中的对称性 ？ \n",
    "\n",
    "tsp问题 ： A→B→C→D→E→A 等价于  B→C→D→E→A→B\n",
    "\n",
    "kp问题 ： A→B→C 等价于  C→B→A\n",
    "\n",
    "CVRP问题  ：车辆顺序的可替换性，连接客户可以从不同起点开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q  ： 如果加上了配送中心限制或者是充电站限制，还能够用上面方法进行数据增强吗 ？ \n",
    "\n",
    "可，将配送中心和充电站也进行旋转替换 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图表 ： \n",
    "\n",
    "tradition vs parallel\n",
    "\n",
    "![传统与并行对比](./POMO_graph/tradition%20vs%20parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统策略 ：单一起点开始，按照policy network顺序生成决策序列，每个决策步骤，依赖前一步的输出\n",
    "\n",
    "并行策略 ：从不同的起点开始，多个序列同时运行，所有序列共享一个参数，这样进行参数优化的时候就可以降低方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pomo training\n",
    "\n",
    "![pomo training](./POMO_graph/pomo_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重点还是同一个问题的多个轨迹共享一个基线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pomo inference\n",
    "\n",
    "![pomo inference](./POMO_graph/pomo_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理步骤 ：\n",
    "\n",
    "问题增强，就是通过坐标旋转等方式，将问题转换成k个问题实例\n",
    "\n",
    "起点选择 ：进行起点选择对数据增强\n",
    "\n",
    "解生成 ：按照训练好的策略网格，对网络进行确定性贪婪推理，每个起点都形成一个方案\n",
    "\n",
    "解选择 ：选择回报最高的解作为最终解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感受\n",
    "\n",
    "最大的感受就是这个pomo算法基本上可以用在每个co问题上面，解决了数据量不足的问题，这种通过旋转，选择不同起点来数据增强的思想真的很棒且适用性很广"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
