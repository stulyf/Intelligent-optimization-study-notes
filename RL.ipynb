{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习(reforcement learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是强化学习 ？ \n",
    "\n",
    "强化学习是机器学习的一个分支，它通过研究智能体在环境中通过试错学习来最大化积累奖励，通过行动（Action）来获得反馈（reward），从这些经验获取最优策略，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程 （MDP） ： 马尔可夫决策过程是强化学习的数学框架，用于描述智能体与环境交互的决策问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP是一个五元组 $(S, A, P, R, \\gamma)$：\n",
    "- $S$：状态空间，所有可能状态的集合\n",
    "- $A$：动作空间，所有可能动作的集合\n",
    "- $P$：状态转移概率函数，$P(s'|s,a)$ 表示在状态$s$下执行动作$a$后转移到状态$s'$的概率\n",
    "- $R$：奖励函数，$R(s,a,s')$ 表示从状态$s$执行动作$a$转移到状态$s'$获得的奖励\n",
    "- $\\gamma$：折扣因子，用于平衡即时奖励与未来奖励的重要性，取值范围为[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些强化学习的基本算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是贝尔曼状态方程？\n",
    "\n",
    "贝尔曼方程描述了最优控制问题中的递归方式，特别是在MDP中，这个方程建立了当前状态的值和未来状态值之间的关系\n",
    "\n",
    "贝尔曼方程的核心就是最优子结构原理：一个问题的最优解包含其子问题的最优解\n",
    "\n",
    "状态值函数的贝尔曼期望方程：\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$$\n",
    "\n",
    "动作值函数的贝尔曼期望方程： \n",
    "\n",
    "$$Q^{\\pi}(s,a) = \\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是策略迭代？\n",
    "\n",
    "策略迭代的核心思想是 ： 先评估当前策略的价值，然后基于这个价值函数改进策略，如此反复直到策略稳定\n",
    "\n",
    "策略迭代算法分为两个部分，一个是对当前策略价值的估计\n",
    "\n",
    "$$V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V(s')]$$\n",
    "\n",
    "这个方程基于贝尔曼期望方程，表示在策略$\\pi$ 下，状态s的价值等于改状态下采取各种动作的期望汇报\n",
    "\n",
    "一个是对当前策略的改进 ： \n",
    "\n",
    "$$\\pi'(s) \\leftarrow \\arg\\max_{a} \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V(s')]$$\n",
    "\n",
    "终止条件是当函数收敛到全局最优的时候，就能返回最优策略和最优价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是值迭代 ？ \n",
    "\n",
    "通过贝尔曼最优方程直接迭代更新值函数，直到收敛，然后从最优值函数中提取最优策略\n",
    "\n",
    "贝尔曼最优方程 ：\n",
    "\n",
    "$$V(s) = \\max_a \\sum{s'} P(s'|s,a) [R(s,a,s') + \\gamma V(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是Q-Learning ? \n",
    "\n",
    "Q-Learning的核心思想是学习一个动作价值函数（Q函数），该函数估计给特定状态下采取特定动作，然后遵循最优策略能够获得的期望累计奖励，通过不断与环境交互并更新这个Q函数，智能体能够最终学习到最优策略\n",
    "\n",
    "Q函数\n",
    "\n",
    "Q函数表示在状态s下执行动作a，然后遵循最优策略$\\pi$所能够获得的期望累计奖励\n",
    "\n",
    "$$Q(s,a) = \\mathbb{E}[R{t+1} + \\gamma \\max_{a'} Q(S{t+1}, a') | S_t = s, A_t = a]$$\n",
    "\n",
    "Q-learning 的优点 ： \n",
    "\n",
    "不需要环境模型 ： 环境变量指的是在给定当前状态和动作的情况下，下一个状态和奖励是什么函数，具体来说，环境模型包括：状态转移概率函数、奖励函数\n",
    "\n",
    "能够处理离散动作空间，但是无法处理连续动作空间\n",
    "\n",
    "离策略学习 \n",
    "\n",
    "即行为策略和目标策略不同，和on-policy最大的不同在于在策略只能够从当前策略生成的数据学习，可以从任何来源的数据学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是策略梯度方法？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etc  ； REINFORCE、TD3、DDPG、SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和vrp问题的一些结合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是vrp问题？\n",
    "\n",
    "vrp问题是一类组合优化问题，其通过确定一组车辆的最优路径，使得这些车辆从一个或者多个配送中心出发，服务分散在不同地理位置的客户，最终返回配送中心，同时满足特定的约束条件\n",
    "\n",
    "常见的vrp问题约束： \n",
    "\n",
    "k辆相同或者是不同的车辆，每个车辆由特定的容量限制，负载不能超过其容量\n",
    "\n",
    "客户需求 ；每个客户都有一个非负需求量，每个客户必须且智能被车访问一次\n",
    "\n",
    "消除子回路\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理大规模vrp问题的时候，我们可以用到强化学习的什么算法，为什么？\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
