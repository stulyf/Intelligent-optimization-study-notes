{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title :  Attention , learning to solve routing problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一遍阅读 ： \n",
    "\n",
    "摘要部分 ： 利用机器学习方法来发现组合优化问题的启发式算法是一个有前景的研究方向，但是还是面临两个问题，一个是需要更好的模型架构，一个是需要更加有效的训练方法。这个论文做的两个工作就是提出了一个基于注意力层的模型，模型较于之前的点网络有着显著的优势，一个是使用reinforce算法结合“确定贪婪回滚”基线进行训练，实验的结果是显著改进了旅行商问题的学习型启发式算法性能，最终达到的结果是优于基础基线，同时为所有的特定模型提供了一个通用的方法\n",
    "\n",
    "对于摘要的问题 ： \n",
    "\n",
    "什么是reinforce？什么是点网络？所以上述算法是只需要我们提供数据，然后自行拟合出启发式算法是吗？\n",
    "\n",
    "结论部分 ： 这篇论文觉得自己的贡献主要在于显著改进了学习型启发式算法在旅行商问题上面的表现，同时这个论文提出的方法在多论文上面的通用性很好。与之前基于LSTM的递归方法相比，注意力机制提高了学习效率，同时也在信息传递上提供了不同的通道交换信息，论文指出了两个未来的研究方向，扩展到更大规模的问题，处理复杂约束条件\n",
    "\n",
    "对结论的问题？\n",
    "\n",
    "什么是LSTM ？ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图表\n",
    "\n",
    "Encoder\n",
    "\n",
    "![Encoder](AM_graph/enocodergraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器第一层是线性映射，通过线性变化将低维向量映射到高维向量中\n",
    "\n",
    "第二层是注意力机制 : 输入序列中的每个元素都被转化为查询(Q)、键(K)和值(V)三种向量,通过计算查询和键的点积，确定词与词之间的关注度，这些关注度决定了每个位置该从其他位置汲取信息\n",
    "\n",
    "第三层是FF前馈网络层：添加非线性部分，同时第二层到第三层之间添加了双通道，就是残差连接\n",
    "\n",
    "最后一层之间指的是最后降到输出维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder\n",
    "\n",
    "!![Encoder](AM_graph/decodergraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = 1 : 首先创建初始上下文，生成初始状态，然后通过mha来生成查询变量，计算查询向量和所有节点嵌入的兼容性,生成选择每个节点的概率分布，用softmax函数，然后选择概率最高的作为下一站\n",
    "\n",
    "后序步骤和第一步基本相同，只是添加了掩码处理，使得只考虑未访问的节点\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reinforce\n",
    "\n",
    "![reforce](AM_graph/reinforcegraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE with Rollout Baseline\n",
    "\n",
    "输入参数是 ： 训练伦次，每轮步数，批量大小，统计显著性阙值\n",
    "\n",
    "初始化 ： 模型参数\n",
    "\n",
    "生成训练样本 ： 随机生成b个问题实例\n",
    "\n",
    "使用当前策略解决方案i对每个实例解决\n",
    "\n",
    "使用基线解进行贪婪回滚，获得方案\n",
    "\n",
    "计算策略梯度 ： 对于每个项目有着不同的L(π)函数，对tsp问题是距离，使用二者成本差L(π)-L(π_BL)计算梯度\n",
    "\n",
    "参数更新\n",
    "\n",
    "比较性能，当性能差距p小于显著性差异的时候，对极限策略参数更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result\n",
    "\n",
    "![result](AM_graph/resultgraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result  :\n",
    "\n",
    "对tsp，scrp，sdvsp（分批配送），op（定向越野）， pctsp（奖品收集）问题上面基本上要么是接近sota，要么是和sota差距不大，同时这个算法又有很好的泛化性，这就显出他的独特优势了\n",
    "\n",
    "AM (greedy)：高效，贪婪\n",
    "\n",
    "AM (sampling)：抽样，时间稍长，但是质量更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AM vs PN\n",
    "\n",
    "![AM vs PN](AM_graph/AMvsPN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "am 普遍优于 pn \n",
    "\n",
    "pn ： 用rnn的架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感受\n",
    "\n",
    "可能存在的创新方向，就是将am算法应用在特定领域上面，像这篇AAMAS论文一样：https://github.com/ntt-dkiku/evrp-eps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
